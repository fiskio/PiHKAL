% 2016-01-12T13:33:50+00:00
% Encoding: UTF8

@Article{1412.7753,
  title = {Learning Longer Memory in Recurrent Neural Networks},
  author = {Mikolov, Tomas and Joulin, Armand and Chopra, Sumit and Mathieu, Michael and Ranzato, {Marc'Aurelio}},
  year = {2015},
  eprint = {1412.7753v2},
  month = apr,
  url = {http://arxiv.org/abs/1412.7753v2},
  file = {:papers/1412.7753.pdf:PDF},
  added = {2015-10-26},
  arxiv = {1412.7753v2},
  arxivcategories = {cs.NE cs.LG},
  primaryclass = {cs.NE},
  journal = {ArXiv e-prints},
  archiveprefix = {arXiv},
  arxivcreated = {2014-12-24},
  arxivupdated = {2015-04-16},
  shortjournal = {arXiv},
  abstract = {Recurrent neural network is a powerful model that learns temporal patterns in
sequential data. For a long time, it was believed that recurrent networks are
difficult to train using simple optimizers, such as stochastic gradient
descent, due to the so-called vanishing gradient problem. In this paper, we
show that learning longer term patterns in real data, such as in natural
language, is perfectly possible using gradient descent. This is achieved by
using a slight structural modification of the simple recurrent neural network
architecture. We encourage some of the hidden units to change their state
slowly by making part of the recurrent weight matrix close to identity, thus
forming kind of a longer term memory. We evaluate our model in language
modeling experiments, where we obtain similar performance to the much more
complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber,
1997).}
}

@Article{1409.3215,
  title = {Sequence to Sequence Learning with Neural Networks},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, {Quoc V.}},
  year = {2014},
  eprint = {1409.3215v3},
  month = dec,
  url = {http://arxiv.org/abs/1409.3215v3},
  added = {2015-10-26},
  archiveprefix = {arXiv},
  arxiv = {1409.3215v3},
  arxivcategories = {cs.CL cs.LG},
  arxivcreated = {2014-09-10},
  arxivupdated = {2014-12-14},
  comments = {9 pages},
  file = {:papers/1409.3215.pdf:PDF},
  journal = {ArXiv e-prints},
  primaryclass = {cs.CL},
  shortjournal = {arXiv},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent
performance on difficult learning tasks. Although DNNs work well whenever large
labeled training sets are available, they cannot be used to map sequences to
sequences. In this paper, we present a general end-to-end approach to sequence
learning that makes minimal assumptions on the sequence structure. Our method
uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to
a vector of a fixed dimensionality, and then another deep LSTM to decode the
target sequence from the vector. Our main result is that on an English to
French translation task from the WMT'14 dataset, the translations produced by
the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's
BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did
not have difficulty on long sentences. For comparison, a phrase-based SMT
system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM
to rerank the 1000 hypotheses produced by the aforementioned SMT system, its
BLEU score increases to 36.5, which is close to the previous best result on
this task. The LSTM also learned sensible phrase and sentence representations
that are sensitive to word order and are relatively invariant to the active and
the passive voice. Finally, we found that reversing the order of the words in
all source sentences (but not target sentences) improved the LSTM's performance
markedly, because doing so introduced many short term dependencies between the
source and the target sentence which made the optimization problem easier.}
}

@ARTICLE{1508.05051,
  file = {:papers/1508.05051.pdf:PDF},
  arxiv = {1508.05051v1},
  title = {Auto-Sizing Neural Networks: With Applications to n-gram Language Models},
  arxivcategories = {cs.CL},
  primaryclass = {cs.CL},
  author = {Murray, Kenton and Chiang, David},
  journal = {ArXiv e-prints},
  eprint = {1508.05051v1},
  archiveprefix = {arXiv},
  arxivcreated = {2015-08-20},
  year = {2015},
  month = aug,
  comments = {EMNLP 2015},
  url = {http://arxiv.org/abs/1508.05051v1},
  added = {2015-10-29},
  shortjournal = {arXiv},
  abstract = {Neural networks have been shown to improve performance across a range of
natural-language tasks. However, designing and training them can be
complicated. Frequently, researchers resort to repeated experimentation to pick
optimal settings. In this paper, we address the issue of choosing the correct
number of units in hidden layers. We introduce a method for automatically
adjusting network size by pruning out hidden units through $\ell_{\infty,1}$
and $\ell_{2,1}$ regularization. We apply this method to language modeling and
demonstrate its ability to correctly choose the number of hidden units while
maintaining perplexity. We also include these models in a machine translation
decoder and show that these smaller neural models maintain the significant
improvements of their unpruned versions.}
}

@inproceedings{DBLP:conf/icml/JozefowiczZS15,
  author = {Rafal J{\'o}zefowicz and
      Wojciech Zaremba and
         Ilya Sutskever},
  title = {An Empirical Exploration of Recurrent Network Architectures},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning,
      {ICML} 2015, Lille, France, 6-11 July 2015},
  pages = {2342--2350},
  year = {2015},
  crossref = {DBLP:conf/icml/2015},
  url = {http://jmlr.org/proceedings/papers/v37/jozefowicz15.html},
  timestamp = {Sun, 05 Jul 2015 19:10:23 +0200},
  biburl = {http://dblp.uni-trier.de/rec/bib/conf/icml/JozefowiczZS15},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  added = {2015-10-30}
}

@ARTICLE{1506.06726,
  file = {:papers/1506.06726.pdf:PDF},
  arxiv = {1506.06726v1},
  title = {Skip-Thought Vectors},
  arxivcategories = {cs.CL cs.LG},
  primaryclass = {cs.CL},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, {Richard S.} and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  journal = {ArXiv e-prints},
  eprint = {1506.06726v1},
  archiveprefix = {arXiv},
  arxivcreated = {2015-06-22},
  year = {2015},
  month = jun,
  comments = {11 pages},
  url = {http://arxiv.org/abs/1506.06726v1},
  added = {2015-11-02},
  shortjournal = {arXiv},
  abstract = {We describe an approach for unsupervised learning of a generic, distributed
sentence encoder. Using the continuity of text from books, we train an
encoder-decoder model that tries to reconstruct the surrounding sentences of an
encoded passage. Sentences that share semantic and syntactic properties are
thus mapped to similar vector representations. We next introduce a simple
vocabulary expansion method to encode words that were not seen as part of
training, allowing us to expand our vocabulary to a million words. After
training our model, we extract and evaluate our vectors with linear models on 8
tasks: semantic relatedness, paraphrase detection, image-sentence ranking,
question-type classification and 4 benchmark sentiment and subjectivity
datasets. The end result is an off-the-shelf encoder that can produce highly
generic sentence representations that are robust and perform well in practice.
We will make our encoder publicly available.}
}

@ARTICLE{1512.04906,
  file = {:papers/1512.04906.pdf:PDF},
  arxiv = {1512.04906v1},
  title = {Strategies for Training Large Vocabulary Neural Language Models},
  arxivcategories = {cs.CL cs.LG},
  primaryclass = {cs.CL},
  author = {Chen, Welin and Grangier, David and Auli, Michael},
  journal = {ArXiv e-prints},
  eprint = {1512.04906v1},
  archiveprefix = {arXiv},
  arxivcreated = {2015-12-15},
  year = {2015},
  month = dec,
  comments = {12 pages; journal paper; under review},
  url = {http://arxiv.org/abs/1512.04906v1},
  added = {2015-12-21},
  shortjournal = {arXiv},
  abstract = {Training neural network language models over large vocabularies is still
computationally very costly compared to count-based models such as Kneser-Ney.
At the same time, neural language models are gaining popularity for many
applications such as speech recognition and machine translation whose success
depends on scalability. We present a systematic comparison of strategies to
represent and train large vocabularies, including softmax, hierarchical
softmax, target sampling, noise contrastive estimation and self normalization.
We further extend self normalization to be a proper estimator of likelihood and
introduce an efficient variant of softmax. We evaluate each method on three
popular benchmarks, examining performance on rare words, the speed/accuracy
trade-off and complementarity to Kneser-Ney.}
}

@ARTICLE{1601.02539,
  file = {:papers/1601.02539.pdf:PDF},
  arxiv = {1601.02539v1},
  title = {Investigating gated recurrent neural networks for speech synthesis},
  arxivcategories = {cs.CL cs.NE},
  primaryclass = {cs.CL},
  author = {Wu, Zhizheng and King, Simon},
  journal = {ArXiv e-prints},
  eprint = {1601.02539v1},
  archiveprefix = {arXiv},
  arxivcreated = {2016-01-11},
  year = {2016},
  month = jan,
  comments = {Accepted by ICASSP 2016},
  url = {http://arxiv.org/abs/1601.02539v1},
  added = {2016-01-12},
  shortjournal = {arXiv},
  abstract = {Recently, recurrent neural networks (RNNs) as powerful sequence models have
re-emerged as a potential acoustic model for statistical parametric speech
synthesis (SPSS). The long short-term memory (LSTM) architecture is
particularly attractive because it addresses the vanishing gradient problem in
standard RNNs, making them easier to train. Although recent studies have
demonstrated that LSTMs can achieve significantly better performance on SPSS
than deep feed-forward neural networks, little is known about why. Here we
attempt to answer two questions: a) why do LSTMs work well as a sequence model
for SPSS; b) which component (e.g., input gate, output gate, forget gate) is
most important. We present a visual analysis alongside a series of experiments,
resulting in a proposal for a simplified architecture. The simplified
architecture has significantly fewer parameters than an LSTM, thus reducing
generation complexity considerably without degrading quality.}
}

